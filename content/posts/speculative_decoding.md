+++
title = 'Speculative Decoding'
date = 2024-02-25T22:35:42+01:00
draft = false
math = true
+++

Speculative Decoding works by considering two models $M_p$ and $M_q$ where $M_p$ represents the target model(slow or expensive to evaluate) and $M_q$ is a more efficient model for the same task(what would happen if it's not the same task?). The conditional probability $p(x_t|x_{<t}) = p(x)$ is what we get when we run a prefix/context $x_{<t}$ through the model $M_p$. We have the same thing for q.


The algorithm is divided into three steps:


1.   Use $M_q$ to autoregressively generate $\gamma$ guesses
2.   Use $M_p$ to evaluate all the guesses and their probabilities in *parallel* and accept those that give the same distribution
3.   Sample an additional token from an adjusted distribution to fix the first rejected token or to add an additional one if they are all accepeted

Each time the target model is run in parallel, a token is either accepted or generated thus even in the worst case(all completions generated by $M_q$ are rejected), the number of serial runs of the target model can not be larger than that of the simple autoregressive method. But it can generate up to $\gamma + 1$ new tokens depending on how well is $M_q$ an approximaton of $M_p$. This means that choosing $\gamma$ is crucial.

## Speculative Sampling

To sample a token $x$ from $p(x)$, we sample $x$ from $q(x)$. If the acceptance rate $r = \frac{p(x)}{q(x)}$ is bigger than 1 we accept $x$ else we reject it with a probability equal to $1 - r$ and sample x again from an adjusted distribution defined as $p'(x) = norm(max(0, p(x) - q(x))) = \frac{p(x)-min(q(x), p(x))}{\sum_{x'}p(x')-min(q(x'), p(x'))}$.

This ensures that each sample $x$ is indeed from $p(x)$.